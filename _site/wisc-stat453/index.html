<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<title> 威斯康星深度学习和生成模型导论-学习资料 — 威斯康星深度学习和生成模型导论课程，内容覆盖深度学习、计算机视觉、自然语言处理、强化学习等 &raquo;  ShowMeAI</title>
<meta name="description" content="探索机器学习与人工智能的世界">
<meta name="keywords" content="深度学习, 计算机视觉, 自然语言处理, 强化学习">
<link rel="canonical" href="/wisc-stat453/">
        




<!-- Twitter Cards -->
<meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="威斯康星深度学习和生成模型导论-学习资料" />
<meta name="twitter:description" content="探索机器学习与人工智能的世界" />
<meta name="twitter:image" content="" />

<!-- Google plus -->
<meta name="author" content="">
<link rel="author" href="">

<!-- Open Graph -->
<meta property="og:locale" content="zh-CN">
<meta property="og:type" content="article">
<meta property="og:title" content="威斯康星深度学习和生成模型导论-学习资料">
<meta property="og:description" content="探索机器学习与人工智能的世界">
<meta property="og:url" content="/wisc-stat453/">
<meta property="og:site_name" content="ShowMeAI">

        <link href='https://fonts.googleapis.com/css?family=Inconsolata:400,700' rel='stylesheet' type='text/css'>
<link rel="stylesheet" href="/assets/vendor/normalize-css/normalize.css">
<link rel="stylesheet" href="/css/main.css">

  <link rel="stylesheet" href="/assets/vendor/highlight/styles/solarized_dark.css">

<link rel="stylesheet" href="/assets/vendor/font-awesome/css/font-awesome.css">

    </head>

    <body>
        <div class="wrapper">
            <header class="header">
    <div class="navigation">
        <a href="/" class="logo">ShowMeAI</a>

        <ul class="menu">
            <li class="menu__entry"><a href="http://showmeai.tech/">官方网站</a></li>
            <li class="menu__entry"><a href="/tags">内容列表</a></li>
            <li class="menu__entry"><a href="/showmeai-resources-hub">资源大全</a></li>
            <li class="menu__entry"><a href="/showmeai-intro">关于我们</a></li>
        </ul>
    </div>

    <ul class="social-links">
        
            <a href="https://github.com/showmeai-hub" class="social-links__entry" target="_blank">
                <i class="fa fa-github"></i>
            </a>
        

        
    </ul>
</header>

            <h1 class="page-title post-title">
    <div class="page-title__text post-title__text">威斯康星深度学习和生成模型导论-学习资料</div>
    <div class="page-title__subtitle post-title__subtitle">威斯康星深度学习和生成模型导论课程，内容覆盖深度学习、计算机视觉、自然语言处理、强化学习等</div>
</h1>

<div class="post-tags">
  关键词标签: 
  
  
  <a href="/tags/#深度学习">深度学习</a>,
  
  <a href="/tags/#计算机视觉">计算机视觉</a>,
  
  <a href="/tags/#自然语言处理">自然语言处理</a>,
  
  <a href="/tags/#强化学习">强化学习</a>
  
  <br>
</div>

<div class="content">
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

<meta name="viewport" content="width=device-width, initial-scale=1" />

<link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700" rel="stylesheet" type="text/css" />

<link rel="stylesheet" href="/assets/vendor/normalize-css/normalize.css" />

<link rel="stylesheet" href="/css/main.css" />

<link rel="stylesheet" href="/assets/vendor/highlight/styles/solarized_dark.css" />

<link rel="stylesheet" href="/assets/vendor/font-awesome/css/font-awesome.css" />

<link rel="shortcut icon" href="/favicon.ico" />

<h2 id="课程介绍">课程介绍</h2>

<div align="center">
<img src="http://image.showmeai.tech/course/WISC-STAT453-1.png" width="100%" />
</div>

<p>STAT453是威斯康星大学Sebastian Raschka老师讲授的深度学习和生成模型课程，课程从机器学习与深度学习的基础知识开始，给大家系统讲解了深度学习及其在视觉与文本数据中的典型模型与各种应用，并以pytorch为主AI框架进行了实战代码讲解。课程对于帮助大家在深度学习知识储备与实战能力构建都有很大帮助。</p>

<p>课程覆盖的核心知识点包括 人工神经元、多层网络、Python、向量化、NumPy、PyTorch、梯度下降、Google Colab、过拟合和欠拟合、CNN、循环神经网络、长短期记忆、降维、GAN、注意力机制、自注意力、多头注意力、Transformer、BERT、GPT、BART、语言模型、Few-Shot、DistilBert 等。</p>

<hr />

<table id="customers" align="center">
  <tr>
    <th>课时编号</th>
    <th>课时内容</th>
</tr><tr>
    <td>第1.0讲</td>
    <td>深度学习介绍&amp;课程介绍</td>
</tr><tr>
    <td>第1.1.1讲</td>
    <td>课程概述第 1 部分：动机和主题</td>
</tr><tr>
    <td>第1.1.2讲</td>
    <td>课程概述第 2 部分：组织</td>
</tr><tr>
    <td>第1.2讲</td>
    <td>什么是机器学习？</td>
</tr><tr>
    <td>第1.3.1讲</td>
    <td>ML 的广泛类别第 1 部分：监督学习</td>
</tr><tr>
    <td>第1.3.2讲</td>
    <td>ML 的广泛类别第 2 部分：无监督学习</td>
</tr><tr>
    <td>第1.3.3讲</td>
    <td>ML 的广泛类别第 3 部分：强化学习</td>
</tr><tr>
    <td>第1.3.4讲</td>
    <td>ML 的广泛类别第 4 部分：监督学习的特殊案例</td>
</tr><tr>
    <td>第1.4讲</td>
    <td>监督学习工作流程</td>
</tr><tr>
    <td>第1.5讲</td>
    <td>机器学习符号和专有名词</td>
</tr><tr>
    <td>第1.6讲</td>
    <td>关于本课程中使用的实践方面和工具</td>
</tr><tr>
    <td>第2.0讲</td>
    <td>深度学习简史【课程概述】</td>
</tr><tr>
    <td>第2.1讲</td>
    <td>人工神经元</td>
</tr><tr>
    <td>第2.2讲</td>
    <td>多层网络</td>
</tr><tr>
    <td>第2.3讲</td>
    <td>深度学习的起源</td>
</tr><tr>
    <td>第2.4讲</td>
    <td>深度学习硬件和软件一览</td>
</tr><tr>
    <td>第2.5讲</td>
    <td>深度学习的当前趋势</td>
</tr><tr>
    <td>第3.0讲</td>
    <td>感知器讲座概述</td>
</tr><tr>
    <td>第3.1讲</td>
    <td>关于大脑和神经元</td>
</tr><tr>
    <td>第3.2讲</td>
    <td>感知器学习规则</td>
</tr><tr>
    <td>第3.3讲</td>
    <td>Python 中的向量化</td>
</tr><tr>
    <td>第3.4讲</td>
    <td>Python 中使用 NumPy 和 PyTorch 构建感知器</td>
</tr><tr>
    <td>第3.5讲</td>
    <td>感知器背后的几何直觉</td>
</tr><tr>
    <td>第4.0讲</td>
    <td>深度学习的线性代数【课程概述】</td>
</tr><tr>
    <td>第4.1讲</td>
    <td>深度学习中的张量</td>
</tr><tr>
    <td>第4.2讲</td>
    <td>PyTorch 中的张量</td>
</tr><tr>
    <td>第4.3讲</td>
    <td>向量、矩阵和广播</td>
</tr><tr>
    <td>第4.4讲</td>
    <td>神经网络的符号约定</td>
</tr><tr>
    <td>第4.5讲</td>
    <td>PyTorch 中的全连接(线性)层</td>
</tr><tr>
    <td>第5.0讲</td>
    <td>梯度下降【课程概述】</td>
</tr><tr>
    <td>第5.1讲</td>
    <td>在线、批处理和小批量模式</td>
</tr><tr>
    <td>第5.2讲</td>
    <td>感知器和线性回归之间的关系</td>
</tr><tr>
    <td>第5.3讲</td>
    <td>线性回归的迭代训练算法</td>
</tr><tr>
    <td>第5.4- (选修)微积分复习 I讲</td>
    <td>导数</td>
</tr><tr>
    <td>第5.5- (选修)微积分复习 II讲</td>
    <td>梯度</td>
</tr><tr>
    <td>第5.6讲</td>
    <td>理解梯度下降</td>
</tr><tr>
    <td>第5.7讲</td>
    <td>训练自适应线性神经元 (Adaline)</td>
</tr><tr>
    <td>第5.8讲</td>
    <td>Adaline 代码示例</td>
</tr><tr>
    <td>第6.0讲</td>
    <td>PyTorch 中的自动微分【课程概述】</td>
</tr><tr>
    <td>第6.1讲</td>
    <td>了解有关 PyTorch 的更多信息</td>
</tr><tr>
    <td>第6.2讲</td>
    <td>理解基于计算图的自动微分</td>
</tr><tr>
    <td>第6.3讲</td>
    <td>PyTorch 中的自动微分</td>
</tr><tr>
    <td>第6.4讲</td>
    <td>使用 PyTorch 训练 ADALINE</td>
</tr><tr>
    <td>第6.5讲</td>
    <td>深入了解 PyTorch API</td>
</tr><tr>
    <td>第7.0讲</td>
    <td>GPU 资源和 Google Colab</td>
</tr><tr>
    <td>第8.0讲</td>
    <td>逻辑回归【课程概述】</td>
</tr><tr>
    <td>第8.1讲</td>
    <td>逻辑回归作为单层神经网络</td>
</tr><tr>
    <td>第8.2讲</td>
    <td>逻辑回归损失函数</td>
</tr><tr>
    <td>第8.3讲</td>
    <td>Logistic 回归损失导数和训练</td>
</tr><tr>
    <td>第8.4讲</td>
    <td>Logits 和交叉熵</td>
</tr><tr>
    <td>第8.5- PyTorch 中的逻辑回归 讲</td>
    <td>代码示例</td>
</tr><tr>
    <td>第8.6讲</td>
    <td>多项 Logistic 回归/Softmax 回归</td>
</tr><tr>
    <td>第8.7.1讲</td>
    <td>OneHot 编码和多类别交叉熵</td>
</tr><tr>
    <td>第8.7.2讲</td>
    <td>OneHot 编码和多类别交叉熵代码示例</td>
</tr><tr>
    <td>第8.8讲</td>
    <td>梯度下降的 Softmax 回归导数</td>
</tr><tr>
    <td>第8.9讲</td>
    <td>使用 PyTorch 的 Softmax 回归代码示例</td>
</tr><tr>
    <td>第9.0讲</td>
    <td>多层感知器【课程概述】</td>
</tr><tr>
    <td>第9.1讲</td>
    <td>多层感知器结构</td>
</tr><tr>
    <td>第9.2讲</td>
    <td>非线性激活函数</td>
</tr><tr>
    <td>第9.3.1讲</td>
    <td>多层感知器代码第 1/3 部分(幻灯片概述)</td>
</tr><tr>
    <td>第9.3.2讲</td>
    <td>PyTorch Part 2/3 中的多层感知器(Jupyter Notebook)</td>
</tr><tr>
    <td>第9.3.3讲</td>
    <td>PyTorch 第 3/3 部分中的多层感知器(脚本设置)</td>
</tr><tr>
    <td>第9.4讲</td>
    <td>过拟合和欠拟合</td>
</tr><tr>
    <td>第9.5.1讲</td>
    <td>猫狗和自定义数据加载器</td>
</tr><tr>
    <td>第9.5.2讲</td>
    <td>PyTorch 中的自定义数据加载器(代码示例)</td>
</tr><tr>
    <td>第10.0讲</td>
    <td>神经网络的正则化方法【课程概述】</td>
</tr><tr>
    <td>第10.1讲</td>
    <td>减少过拟合的技术</td>
</tr><tr>
    <td>第10.2讲</td>
    <td>PyTorch 中的数据增强</td>
</tr><tr>
    <td>第10.3讲</td>
    <td>提前停止</td>
</tr><tr>
    <td>第10.4讲</td>
    <td>神经网络的 L2 正则化</td>
</tr><tr>
    <td>第10.5.1讲</td>
    <td>Dropout 背后的主要概念</td>
</tr><tr>
    <td>第10.5.2讲</td>
    <td>Dropout 共适应解释</td>
</tr><tr>
    <td>第10.5.3讲</td>
    <td>(选修)Dropout 集成解释</td>
</tr><tr>
    <td>第10.5.4讲</td>
    <td>PyTorch 中的 Dropout</td>
</tr><tr>
    <td>第11.0讲</td>
    <td>输入归一化和权重初始化【课程概述】</td>
</tr><tr>
    <td>第11.1讲</td>
    <td>输入规范化</td>
</tr><tr>
    <td>第11.2讲</td>
    <td>BatchNorm 的工作原理</td>
</tr><tr>
    <td>第11.3讲</td>
    <td>PyTorch 中的 BatchNorm</td>
</tr><tr>
    <td>第11.4讲</td>
    <td>为什么 BatchNorm 有效</td>
</tr><tr>
    <td>第11.5- 权重初始化 讲</td>
    <td>我们为什么要关心？</td>
</tr><tr>
    <td>第11.6讲</td>
    <td>Xavier Glorot 和 Kaiming He 初始化</td>
</tr><tr>
    <td>第11.7讲</td>
    <td>PyTorch 中的权重初始化</td>
</tr><tr>
    <td>第12.0讲</td>
    <td>改进基于梯度下降的优化【课程概述】</td>
</tr><tr>
    <td>第12.1讲</td>
    <td>学习率衰减</td>
</tr><tr>
    <td>第12.2讲</td>
    <td>PyTorch 中的学习率调度器</td>
</tr><tr>
    <td>第12.3讲</td>
    <td>具有动量的新元</td>
</tr><tr>
    <td>第12.4讲</td>
    <td>Adam：结合自适应学习率和动量</td>
</tr><tr>
    <td>第12.5讲</td>
    <td>在 PyTorch 中选择不同的优化器</td>
</tr><tr>
    <td>第12.6讲</td>
    <td>关于优化算法的附加主题和研究</td>
</tr><tr>
    <td>第13.0讲</td>
    <td>卷积网络简介【课程概述】</td>
</tr><tr>
    <td>第13.1讲</td>
    <td>CNN 的常见应用</td>
</tr><tr>
    <td>第13.2讲</td>
    <td>图像分类的挑战</td>
</tr><tr>
    <td>第13.3讲</td>
    <td>卷积神经网络基础</td>
</tr><tr>
    <td>第13.4讲</td>
    <td>卷积滤波器和权重共享</td>
</tr><tr>
    <td>第13.5讲</td>
    <td>互相关与卷积</td>
</tr><tr>
    <td>第13.6讲</td>
    <td>CNN 和反向传播</td>
</tr><tr>
    <td>第13.7讲</td>
    <td>CNN 架构和 AlexNet</td>
</tr><tr>
    <td>第13.8讲</td>
    <td>CNN 能看到什么</td>
</tr><tr>
    <td>第13.9.1讲</td>
    <td>PyTorch 中的 LeNet-5</td>
</tr><tr>
    <td>第13.9.2讲</td>
    <td>在 PyTorch 中保存和加载模型</td>
</tr><tr>
    <td>第13.9.3讲</td>
    <td>PyTorch 中的 AlexNet</td>
</tr><tr>
    <td>第14.0讲</td>
    <td>卷积神经网络架构【课程概述】</td>
</tr><tr>
    <td>第14.1讲</td>
    <td>卷积和填充</td>
</tr><tr>
    <td>第14.2讲</td>
    <td>空间丢失和 BatchNorm</td>
</tr><tr>
    <td>第14.3讲</td>
    <td>架构概述</td>
</tr><tr>
    <td>第14.3.1.1讲</td>
    <td>VGG16 概述</td>
</tr><tr>
    <td>第14.3.1.2讲</td>
    <td>PyTorch 中的 VGG16</td>
</tr><tr>
    <td>第14.3.2.1讲</td>
    <td>ResNet 概述</td>
</tr><tr>
    <td>第14.3.2.2讲</td>
    <td>PyTorch 中的 ResNet-34</td>
</tr><tr>
    <td>第14.4.1讲</td>
    <td>用卷积层代替最大池化</td>
</tr><tr>
    <td>第14.4.2讲</td>
    <td>PyTorch 中的全卷积网络</td>
</tr><tr>
    <td>第14.5讲</td>
    <td>卷积而不是全连接层</td>
</tr><tr>
    <td>第14.6.1讲</td>
    <td>迁移学习</td>
</tr><tr>
    <td>第14.6.2讲</td>
    <td>PyTorch 中的迁移学习</td>
</tr><tr>
    <td>第15.0讲</td>
    <td>循环神经网络简介【课程概述】</td>
</tr><tr>
    <td>第15.1讲</td>
    <td>处理文本数据的不同方法</td>
</tr><tr>
    <td>第15.2讲</td>
    <td>使用 RNN 进行序列建模</td>
</tr><tr>
    <td>第15.3讲</td>
    <td>不同类型的序列建模任务</td>
</tr><tr>
    <td>第15.4讲</td>
    <td>时间反向传播概述</td>
</tr><tr>
    <td>第15.5讲</td>
    <td>长短期记忆</td>
</tr><tr>
    <td>第15.6讲</td>
    <td>用于分类的 RNN：多对一结构RNN</td>
</tr><tr>
    <td>第15.7讲</td>
    <td>PyTorch 中的 RNN 情感分类器</td>
</tr><tr>
    <td>第16.0讲</td>
    <td>自编码器简介【课程概述】</td>
</tr><tr>
    <td>第16.1讲</td>
    <td>降维</td>
</tr><tr>
    <td>第16.2讲</td>
    <td>完全连接的自编码器</td>
</tr><tr>
    <td>第16.3讲</td>
    <td>卷积自编码器和转置卷积</td>
</tr><tr>
    <td>第16.4讲</td>
    <td>PyTorch 中的卷积自动编码器</td>
</tr><tr>
    <td>第16.5讲</td>
    <td>其他类型的自动编码器</td>
</tr><tr>
    <td>第17.0讲</td>
    <td>变分自编码器简介【课程概述】</td>
</tr><tr>
    <td>第17.1讲</td>
    <td>变分自编码器概述</td>
</tr><tr>
    <td>第17.2讲</td>
    <td>从变分自动编码器采样</td>
</tr><tr>
    <td>第17.3讲</td>
    <td>Log-Var 技巧</td>
</tr><tr>
    <td>第17.4讲</td>
    <td>变分自编码器损失函数</td>
</tr><tr>
    <td>第17.5讲</td>
    <td>PyTorch 中手写数字的变分自动编码器</td>
</tr><tr>
    <td>第17.6讲</td>
    <td>PyTorch 中人脸图像的变分自动编码器</td>
</tr><tr>
    <td>第17.7讲</td>
    <td>PyTorch 中的 VAE 潜在空间算法</td>
</tr><tr>
    <td>第18.0讲</td>
    <td>生成对抗网络简介</td>
</tr><tr>
    <td>第18.1讲</td>
    <td>GAN 背后的主要思想</td>
</tr><tr>
    <td>第18.2讲</td>
    <td>GAN 目标</td>
</tr><tr>
    <td>第18.3讲</td>
    <td>为实际使用修改 GAN 损失函数</td>
</tr><tr>
    <td>第18.4讲</td>
    <td>在 PyTorch 中生成手写数字的 GAN</td>
</tr><tr>
    <td>第18.5讲</td>
    <td>让 GAN 发挥作用的技巧和窍门</td>
</tr><tr>
    <td>第18.6讲</td>
    <td>在 PyTorch 中生成人脸图像的 DCGAN</td>
</tr><tr>
    <td>第19.0讲</td>
    <td>用于序列到序列建模的 RNN 和转换器</td>
</tr><tr>
    <td>第19.1讲</td>
    <td>使用单词和字符 RNN 生成序列</td>
</tr><tr>
    <td>第19.2.1讲</td>
    <td>在 PyTorch 中实现字符 RNN(概念)</td>
</tr><tr>
    <td>第19.2.2讲</td>
    <td>在 PyTorch 中实现字符 RNN(代码示例)</td>
</tr><tr>
    <td>第19.3讲</td>
    <td>具有注意力机制的 RNN</td>
</tr><tr>
    <td>第19.4.1讲</td>
    <td>在没有 RNN 的情况下使用注意力：一种基本的自我注意力形式</td>
</tr><tr>
    <td>第19.4.2讲</td>
    <td>自注意力和缩放点积注意力</td>
</tr><tr>
    <td>第19.4.3讲</td>
    <td>多头注意力</td>
</tr><tr>
    <td>第19.5.1讲</td>
    <td>Transformer 架构</td>
</tr><tr>
    <td>第19.5.2.1讲</td>
    <td>一些流行的Transformer 模型：BERT、GPT 和 BART【概述】</td>
</tr><tr>
    <td>第19.5.2.2讲</td>
    <td>GPT-v1：成式预训练transformer</td>
</tr><tr>
    <td>第19.5.2.3- BERT讲</td>
    <td>来自transformer的双向编码器表示</td>
</tr><tr>
    <td>第19.5.2.4讲</td>
    <td>GPT-v2：语言模型是无监督的多任务学习者</td>
</tr><tr>
    <td>第19.5.2.5讲</td>
    <td>GPT-v3：语言模型是少数据学习器(Few-Shot Learner)</td>
</tr><tr>
    <td>第19.5.2.6- BART讲</td>
    <td>结合双向和自回归transformer</td>
</tr><tr>
    <td>第19.5.2.7- 结束语 讲</td>
    <td>transformers的近期增长趋势</td>
</tr><tr>
    <td>第19.6讲</td>
    <td>基于PyTorch 的 DistilBert 电影评论分类器</td>
</tr>
</table>

<h2 id="ShowMeAI课程解读">ShowMeAI课程解读：全套资料</h2>
<div align="center">
<img src="http://image.showmeai.tech/course/WISC-STAT453-2.png" width="100%" />
</div>
<h2 id="更多技术与课程清单--点击查看详细课程">更多技术与课程清单 | 点击查看详细课程</h2>
<style>
#customers {
  font-family: Arial, Helvetica, sans-serif;
  border-collapse: collapse;
  width: 80%;
}

#customers td, #customers th {
  border: 2px solid #ddd;
  padding: 8px;
}

#customers tr:hover {background-color: #ddd;}

#customers th {
  padding-top: 12px;
  padding-bottom: 12px;
  text-align: left;
  background-color: #F5B041;
  color: white;
}

</style>

<table id="customers" align="center">
    <tr>
        <th>技术方向</th>
        <th>课程及链接</th>
    </tr>
    <tr>
        <td rowspan="5">计算机数学基础</td>
        <td><a href="/mit-6.042j">MIT-计算机科学的数学基础</a></td>
    </tr>
    <tr>
        <td><a href="/uc-math100">辛辛那提大学-微积分I</a></td>
    </tr>
    <tr>
        <td><a href="/uc-math101">辛辛那提大学-微积分II</a></td>
    </tr>
    <tr>
        <td><a href="/uc-math1071">辛辛那提大学-离散数学</a></td>
    </tr>
    <tr>
        <td><a href="/stanford-engr108">斯坦福-线性代数与矩阵方法导论</a></td>
    </tr>
    <tr>
        <td rowspan="3">计算机科学导论</td>
        <td><a href="/stanford-cs105">斯坦福-计算机科学导论</a></td>
    </tr>
    <tr>
        <td><a href="/harvard-cs50-cs">哈佛-计算机科学导论</a></td>
    </tr>
    <tr>
        <td><a href="/mit-6.0001">MIT-计算机科学与Python编程导论</a></td>
    </tr>
    <tr>
        <td rowspan="2">数据结构与算法</td>
        <td><a href="/mit-6.046j">MIT-数据结构与算法设计</a></td>
    </tr>
    <tr>
        <td><a href="/umd-cmsc420-0101">马里兰大学-数据结构</a></td>
    </tr>
    <tr>
        <td rowspan="2">数据库</td>
        <td><a href="/cmu-14-455">CMU-数据库系统导论</a></td>
    </tr>
    <tr>
        <td><a href="/cmu-15-721">CMU-数据库系统进阶</a></td>
    </tr>
    <tr>
        <td rowspan="2">机器学习及应用</td>
        <td><a href="/cs229">斯坦福CS229</a></td>
    </tr>
    <tr>
        <td><a href="/mit-6.036">MIT-机器学习导论</a></td>
    </tr>
    <tr>
        <td rowspan="8">深度学习及应用</td>
        <td><a href="/cs230">斯坦福CS230</a></td>
    </tr>
    <tr>
        <td><a href="/harvard-cs50-ai">哈佛-Python人工智能入门</a></td>
    </tr>
    <tr>
        <td><a href="/mit-6.s191">MIT-深度学习导论</a></td>
    </tr>
    <tr>
        <td><a href="/ntu-hylee-ml">李宏毅-机器学习(&amp;深度学习)</a></td>
    </tr>
    <tr>
        <td><a href="/tech-adl">应用深度学习(全知识点覆盖)</a></td>
    </tr>
    <tr>
        <td><a href="/berkeley-csw182">UC Berkeley-深度神经网络设计、可视化与理解</a></td>
    </tr>
    <tr>
        <td><a href="/wisc-stat453">威斯康星-深度学习和生成模型导论</a></td>
    </tr>
    <tr>
        <td><a href="/berkeley-fsdl">UC Berkeley-全栈深度学习训练营</a></td>
    </tr>
    <tr>
        <td rowspan="4">自然语言处理</td>
        <td><a href="/cs224n">斯坦福CS224n（深度学习与NLP）</a></td>
    </tr>
    <tr>
        <td><a href="/cs124">斯坦福CS124（从语言到信息）</a></td>
    </tr>
    <tr>
        <td><a href="/cs520">斯坦福CS520（知识图谱）</a></td>
    </tr>
    <tr>
        <td><a href="/umass-cs685">马萨诸塞-自然语言处理进阶</a></td>
    </tr>
    <tr>
        <td rowspan="3">计算机视觉</td>
        <td><a href="/cs231n">斯坦福CS231n（深度学习与CV）</a></td>
    </tr>
    <tr>
        <td><a href="/eecs498">密歇根eecs498（CS231n进阶课）</a></td>
    </tr>
    <tr>
        <td><a href="/adl4cv">慕尼黑工大adl4cv（深度学习与CV高阶课）</a></td>
    </tr>
    <tr>
        <td>多模态</td>
        <td><a href="/cmu-11-777">CMU-多模态机器学习</a></td>
    </tr>
    <tr>
        <td>图机器学习</td>
        <td><a href="/cs224w">斯坦福CS224w</a></td>
    </tr>
    <tr>
        <td rowspan="2">强化学习</td>
        <td><a href="/cs234">斯坦福CS234（强化学习）</a></td>
    </tr>
    <tr>
        <td><a href="/cs285">伯克利CS285（深度强化学习）</a></td>
    </tr>
    <tr>
        <td>无监督学习</td>
        <td><a href="/cs294-158">伯克利CS294-158（深度无监督学习）</a></td>
    </tr>
    <tr>
        <td rowspan="3">AI与生物医疗</td>
        <td><a href="/mit-6.874">MIT-面向生命科学的深度学习</a></td>
    </tr>
    <tr>
        <td><a href="/mit-6.047">MIT-基因组学机器学习</a></td>
    </tr>
    <tr>
        <td><a href="/mit-6.s897">MIT-医疗机器学习</a></td>
    </tr>
    <tr>
        <td rowspan="2">图形学与几何</td>
        <td><a href="/cmu-15-462">CMU-计算机图形学</a></td>
    </tr>
    <tr>
        <td><a href="/ammi-gml">AMMI-几何深度学习</a></td>
    </tr>
    <tr>
        <td rowspan="2">其他课程</td>
        <td><a href="/harvard-cs50-web">哈佛-基于Python/JavaScript的web编程</a></td>
    </tr>
    <tr>
        <td><a href="/mit-18.s191">MIT-计算思维导论(Julia)</a></td>
    </tr>
</table>

<div align="center">
<img src="http://image.showmeai.tech/course/WISC-STAT453-3.png" width="100%" alt="end" align="center" />
</div>

</div>

<div class="about">
    <div class="about__devider">*****</div>
    <div class="about__text">
        本文作者 <strong>  韩信子 </strong><br>
        欢迎关注微信公众号<strong> ShowMeAI研究中心 </strong>获取更多资源！
    </div>
</div>


        </div>

        <script src="/assets/vendor/highlight/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>
        
    </body>
</html>